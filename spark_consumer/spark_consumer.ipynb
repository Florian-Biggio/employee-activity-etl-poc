{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec9ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_json, col\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize Spark with Redpanda config\u001b[39;00m\n\u001b[32m      5\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRedpandaConsumer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.jars.packages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Read from Redpanda\u001b[39;00m\n\u001b[32m     11\u001b[39m df = spark.readStream \\\n\u001b[32m     12\u001b[39m     .format(\u001b[33m\"\u001b[39m\u001b[33mkafka\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     13\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mkafka.bootstrap.servers\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlocalhost:9092\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     14\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33msubscribe\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpg_cdc.public.employee_activities\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     15\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mstartingOffsets\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mearliest\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m     16\u001b[39m     .load()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev\\Desktop\\OpenClassrooms\\Project12\\employee-activity-etl-poc\\venv\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev\\Desktop\\OpenClassrooms\\Project12\\employee-activity-etl-poc\\venv\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev\\Desktop\\OpenClassrooms\\Project12\\employee-activity-etl-poc\\venv\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev\\Desktop\\OpenClassrooms\\Project12\\employee-activity-etl-poc\\venv\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev\\Desktop\\OpenClassrooms\\Project12\\employee-activity-etl-poc\\venv\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Spark Batch Consumer (Test in Jupyter)\n",
    "# \n",
    "# Requires: `pip install pyspark findspark`\n",
    "\n",
    "# %%\n",
    "import findspark\n",
    "findspark.init()  # Only needed if Spark is locally installed\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "import json\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Initialize Spark Session (Local Mode)\n",
    "\n",
    "# %%\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedpandaTest\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Test Connection to Redpanda\n",
    "\n",
    "# %%\n",
    "test_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"pg_cdc.public.employee_activities\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .limit(5)\n",
    "\n",
    "test_df.show(truncate=False)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Process Sample Batch\n",
    "\n",
    "# %%\n",
    "# Mock schema - replace with your actual schema\n",
    "sample_schema = \"\"\"\n",
    "{\n",
    "  \"type\": \"struct\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"id\", \"type\": \"int\"},\n",
    "    {\"name\": \"user_id\", \"type\": \"int\"},\n",
    "    {\"name\": \"activity\", \"type\": \"string\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Parse messages\n",
    "parsed = test_df.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), sample_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "parsed.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Mock Upload Function\n",
    "# %%\n",
    "def mock_upload(df):\n",
    "    print(\"=== Simulating upload to data warehouse ===\")\n",
    "    df.show()\n",
    "    print(f\"Would upload {df.count()} records\")\n",
    "    \n",
    "mock_upload(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e126d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kafka.vendor.six.moves'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# # Slack Notifier Test (Jupyter)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Requires: `pip install python-dotenv kafka-python requests`\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkafka\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KafkaConsumer\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev\\Desktop\\OpenClassrooms\\Project12\\employee-activity-etl-poc\\venv\\Lib\\site-packages\\kafka\\__init__.py:21\u001b[39m\n\u001b[32m     16\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     18\u001b[39m logging.getLogger(\u001b[34m__name__\u001b[39m).addHandler(NullHandler())\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkafka\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconsumer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KafkaConsumer\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkafka\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconsumer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msubscription_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConsumerRebalanceListener\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkafka\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproducer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KafkaProducer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev\\Desktop\\OpenClassrooms\\Project12\\employee-activity-etl-poc\\venv\\Lib\\site-packages\\kafka\\consumer\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimple\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleConsumer\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultiprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiProcessConsumer\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KafkaConsumer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dev\\Desktop\\OpenClassrooms\\Project12\\employee-activity-etl-poc\\venv\\Lib\\site-packages\\kafka\\consumer\\simple.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkafka\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvendor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m six\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkafka\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvendor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msix\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmoves\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m queue \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     Consumer,\n\u001b[32m     17\u001b[39m     FETCH_DEFAULT_BLOCK_TIMEOUT,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     NO_MESSAGES_WAIT_TIME_SECONDS\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     FetchRequestPayload, KafkaError, OffsetRequestPayload,\n\u001b[32m     29\u001b[39m     ConsumerFetchSizeTooSmall,\n\u001b[32m     30\u001b[39m     UnknownTopicOrPartitionError, NotLeaderForPartitionError,\n\u001b[32m     31\u001b[39m     OffsetOutOfRangeError, FailedPayloadsError, check_error\n\u001b[32m     32\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kafka.vendor.six.moves'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Slack Notifier Test (Jupyter)\n",
    "# \n",
    "# Requires: `pip install python-dotenv kafka-python requests`\n",
    "\n",
    "# %%\n",
    "from kafka import KafkaConsumer\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Mock Slack Webhook\n",
    "\n",
    "# %%\n",
    "class MockSlack:\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def post(self, url, json):\n",
    "        self.messages.append(json['text'])\n",
    "        display(Markdown(f\"**Mock Slack:** {json['text']}\"))\n",
    "        return type('obj', (object,), {'status_code': 200})\n",
    "\n",
    "mock_slack = MockSlack()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Test Consumer (Read Last 5 Messages)\n",
    "\n",
    "# %%\n",
    "consumer = KafkaConsumer(\n",
    "    \"pg_cdc.public.employee_activities\",\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    auto_offset_reset=\"earliest\",  # Read from beginning for test\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    consumer_timeout_ms=5000  # Stop after 5 sec inactivity\n",
    ")\n",
    "\n",
    "# %%\n",
    "print(\"Listening for messages...\")\n",
    "for i, message in enumerate(consumer):\n",
    "    if i >= 5:  # Limit to 5 messages for testing\n",
    "        break\n",
    "        \n",
    "    record = message.value\n",
    "    if record.get('op') == 'c':\n",
    "        msg = f\"New activity: {record['after']['activity']}\"  # Adjust fields\n",
    "        mock_slack.post(\"mock_url\", {\"text\": msg})\n",
    "        \n",
    "print(\"Test complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
